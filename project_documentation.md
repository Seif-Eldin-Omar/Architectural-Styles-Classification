# Architectural Styles Classification

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT) This project aims to classify architectural styles from images using deep learning techniques. It leverages Convolutional Neural Networks (CNNs) to learn distinctive features from various architectural examples and predict their corresponding styles.

## Table of Contents

- [Overview](#overview)
- [Project Goal](#project-goal)
- [Features](#features)
- [Dataset](#dataset)
- [Model Architecture](#model-architecture)
- [Technologies Used](#technologies-used)
- [Setup and Installation](#setup-and-installation)
- [Usage](#usage)
  - [Training the Model](#training-the-model)
  - [Evaluating the Model](#evaluating-the-model)
  - [Making Predictions](#making-predictions)
- [Results](#results)
- [File Structure](#file-structure)
- [Contributing](#contributing)
- [License](#license)
- [Author](#author)
- [Acknowledgments](#acknowledgments)

## Overview

*Provide a more detailed overview of your project. What problem does it solve? Why is it interesting or useful? What are the key architectural styles you are focusing on?*

For example:
This project explores the application of deep learning to the domain of architectural design and history. By training a model to recognize diverse architectural styles such as Gothic, Baroque, Modern, Art Deco, etc., this work can serve as a tool for students, architects, or enthusiasts to identify and learn about different building aesthetics.

## Project Goal

*Clearly state the main objective(s) of this project.*

Example:
The primary goal of this project is to develop and evaluate a robust image classification model capable of accurately identifying a predefined set of architectural styles from input images. Secondary goals include exploring different CNN architectures, data augmentation techniques, and achieving a certain performance benchmark (e.g., accuracy, F1-score).

## Features

*List the key features of your project.*

-   Classification of **[Number]** distinct architectural styles:
    -   *[Style 1]*
    -   *[Style 2]*
    -   *[Style 3]*
    -   *... (List all styles the model can classify)*
-   Implementation using **[Primary Deep Learning Framework, e.g., TensorFlow with Keras, PyTorch]**.
-   Pre-trained model available for inference (if applicable).
-   Scripts for data preprocessing, model training, evaluation, and prediction.
-   Visualization of training progress and results (if applicable).

## Dataset

*Describe the dataset used for training and testing your model. This is a critical section.*

-   **Name of the dataset:** *[e.g., "Architectural Heritage Elements Image Dataset (AHEID)", "Custom Scraped Dataset of Architectural Styles", "A subset of WikiArquitectura"]*
-   **Source:** *[Provide a link to the dataset if public, or describe how it was collected. e.g., Kaggle, specific research paper, self-scraped from Google Images/Pinterest/Flickr]*
-   **Description:**
    -   Total number of images: *[e.g., 10,000 images]*
    -   Number of classes (styles): *[e.g., 10 styles]*
    -   Distribution of images per class: *[e.g., Roughly 1000 images per style, or mention if it's imbalanced]*
    -   Image resolution/format: *[e.g., Varied resolutions, mostly JPG]*
    -   Any preprocessing steps applied to the dataset before use: *[e.g., resizing, cleaning, manual labeling]*
-   **Data Split:**
    -   Training set: *[e.g., 70% of images]*
    -   Validation set: *[e.g., 15% of images]*
    -   Test set: *[e.g., 15% of images]*

*If you created a custom dataset or significantly modified an existing one, provide more details on its creation process.*

## Model Architecture

*Detail the neural network architecture used.*

-   **Base Model:** *[e.g., Custom CNN, ResNet50, VGG16, InceptionV3, EfficientNet, etc.]*
-   **Modifications (if any):** *[e.g., Added custom top layers, used transfer learning with fine-tuning, changed specific layers]*
-   **Key Layers and Components:**
    -   Convolutional layers (number, filter sizes, activation functions like ReLU)
    -   Pooling layers (type, e.g., MaxPooling, AveragePooling)
    -   Dropout layers (rate, for regularization)
    -   Batch Normalization layers
    -   Fully Connected (Dense) layers
    -   Output layer (activation function, e.g., Softmax for multi-class classification)
-   **Optimizer:** *[e.g., Adam, SGD, RMSprop] with learning rate *[e.g., 0.001]*
-   **Loss Function:** *[e.g., Categorical Crossentropy, Sparse Categorical Crossentropy]*
-   **Metrics Tracked:** *[e.g., Accuracy, Precision, Recall, F1-score, Confusion Matrix]*

*You can include a diagram of the model architecture if you have one (e.g., generated by `model.summary()` in Keras and saved as an image).*
```markdown
![Model Architecture](assets/model_architecture.png) ```

## Technologies Used

*List all major libraries, frameworks, and tools used in your project.*

-   **Programming Language:** Python ([Version, e.g., 3.8+])
-   **Deep Learning Framework:** [e.g., TensorFlow (2.x), Keras, PyTorch (1.x)]
-   **Core Libraries:**
    -   NumPy: For numerical operations
    -   Pandas: For data manipulation (if used for dataset handling)
    -   Matplotlib / Seaborn: For plotting and visualizations
    -   Scikit-learn: For evaluation metrics, data splitting
    -   OpenCV (cv2): For image processing tasks
-   **Environment:** [e.g., Jupyter Notebooks, Google Colab, Local IDE (VS Code, PyCharm)]
-   **Other tools:** [e.g., Git, DVC (for data versioning)]

## Setup and Installation

*Provide clear, step-by-step instructions on how to set up the project environment and install dependencies.*

1.  **Clone the repository:**
    ```bash
    git clone [https://www.google.com/search?q=https://github.com/Seif-Eldin-Omar/Architectural-Styles-Classification.git](https://www.google.com/search?q=https://github.com/Seif-Eldin-Omar/Architectural-Styles-Classification.git)
    cd Architectural-Styles-Classification
    ```

2.  **Create a virtual environment (recommended):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows use `venv\Scripts\activate`
    ```

3.  **Install dependencies:**
    *If you have a `requirements.txt` file (highly recommended):*
    ```bash
    pip install -r requirements.txt
    ```
    *If not, list the main packages to install manually:*
    ```bash
    pip install tensorflow numpy pandas matplotlib scikit-learn opencv-python  # Add other necessary packages
    ```

4.  **Download the Dataset:**
    *Provide instructions here. This might involve:*
    -   Running a script: `python download_dataset.py`
    -   Manual download link: "Download the dataset from [Link] and extract it to the `./data/` directory."
    -   If using Kaggle API: `kaggle datasets download -d USERNAME/DATASET_NAME -p ./data/ --unzip`

5.  **Directory Structure:**
    *Briefly explain the expected directory structure, especially for data if it's not automatically created.*
    ```
    Architectural-Styles-Classification/
    ├── data/
    │   ├── train/
    │   │   ├── style1/
    │   │   └── style2/
    │   ├── validation/
    │   └── test/
    ├── notebooks/  # or scripts/
    │   ├── 01_data_preprocessing.ipynb
    │   ├── 02_model_training.ipynb
    │   └── 03_model_evaluation.ipynb
    ├── src/  # if you have .py modules
    │   ├── model.py
    │   └── utils.py
    ├── models/  # for saved model files
    │   └── architectural_style_classifier.h5
    ├── requirements.txt
    └── README.md
    ```

## Usage

*Explain how to run your scripts or notebooks to train the model, evaluate it, and make predictions.*

### Training the Model

*Provide the command or steps to start the training process.*
Example:
"To train the model, run the `02_model_training.ipynb` Jupyter Notebook or execute the training script:"
```bash
python train_model.py --data_dir ./data/ --epochs 50 --batch_size 32
